{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7586aefb",
   "metadata": {},
   "source": [
    "# 3 Differential Equations\n",
    "\n",
    "The key difference between difference equations and differential equations is that the former model a system in discrete time, i.e. the evolution between one point in time and the next, while the latter model a system in continuous time. This raises an important issue for simulations of such models: computers only work in discrete states (1 & 0, ultimately), so \"true\" continuity cannot be implemented. Hence, we have to work with approximations. Usually that creates a trade-off between accuracy (how close is the solutions to the true, e.g. analytically derived solution?) and computational efficiency (how much work does the computer  have to do, and how long is it going to take?).\n",
    "\n",
    "In this tutorial, two different timestepping methods will be introduced:  explicit Euler method and the Runge's central difference quotient method. You will write functions to simulate the behaviour of a first-order differential equation. We can then compare the accuracy of both solutions by calculating the errors relative to the analytical solution, and also plot the behaviour of the system over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4bf1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this renders text in figures in Latex font\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c59f4e",
   "metadata": {},
   "source": [
    "### Taylor expansion, finite differences\n",
    "\n",
    "Differential equations are of the basic form\n",
    "\n",
    "$\\frac{\\partial y}{\\partial t} = y'(t) = f(y(t), t)$,\n",
    "\n",
    "i.e. the rate of change of $y$, its time derivative, is a function of $y$ itself, and sometimes time as well. In order to simulate one time step of size $\\delta$, apply a second-order Taylor expansion and rearrange in the following way:\n",
    "\n",
    "$y(t+\\delta) = y(t) + \\delta y'(t) + \\frac{\\delta^2}{2}y''(\\eta)$\n",
    "\n",
    "$\\frac{y(t+\\delta) - y(t)}{\\delta} = y'(t) + \\frac{\\delta^2}{2}y''(\\eta)$\n",
    "\n",
    "with $\\eta\\in[t, t+\\delta]$. The difference quotient on the left-hand side converges to $y'(t)$ as $\\delta\\rightarrow 0$.\n",
    "\n",
    "### Explicit Euler\n",
    "The first method, the explicit Euler method, applies an approximation of this relationship in a straightforward fashion:\n",
    "\n",
    "$\\frac{y(t+\\delta) - y(t)}{\\delta} \\approx y'(t) = f(y(t),t)$\n",
    "\n",
    "$\\Leftrightarrow y(t+\\delta) \\approx y(t) + \\delta f(y(t), t)$\n",
    "\n",
    "i.e. we move along the tangent line of the function at the current position. In terms of timestepping, this leads to the scheme\n",
    "\n",
    "$y(0) = y_0$\n",
    "\n",
    "$y(\\delta) = y(0) + \\delta f(y(0), 0)$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$y(T) = y(T-\\delta) + \\delta f(y(T-\\delta),T-\\delta)$\n",
    "\n",
    "There is a small error related to the higher order terms of the Taylor expansion, but the smaller $\\delta$, the higher the accuracy we achieve. On the other hand, with smaller $\\delta$, we need more time steps, leading to higher computational effort, as more calculations have to be performed.\n",
    "\n",
    "##### EXERCISE\n",
    "\n",
    "Consider the continuous-time version of the cobweb model (tutorial 3, exercise 2), leading to the first-order differential equation $p'=45 - 3p$. Implement one time step using this scheme! It should take the current price $p$, as well as the size of the timestep $\\delta$ as inputs and return the estimated new price after a step of size $\\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4078379",
   "metadata": {},
   "source": [
    "##### SOLUTION\n",
    "\n",
    "We have the differential equation $p'=45 - 3p$. The timestepping scheme then reads\n",
    "\n",
    "$p_{t+\\delta} = p_t + \\delta p' = p_t + \\delta(45-3p_t) = (1-3\\delta)p_t+45\\delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98bb135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_euler_1step(p, delta):\n",
    "    # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611326d5",
   "metadata": {},
   "source": [
    "### Central difference quotient\n",
    "\n",
    "Instead of applying a Taylor expansion around the point $y(t)$ to approximate $y(t+\\delta)$, we can apply the Taylor expansion in the mid-point between the two, i.e. at $y(t+\\frac{\\delta}{2})$. In that case we obtain expressions for both $y(t)$ and $y(t+\\delta)$:\n",
    "\n",
    "$\\begin{align}\n",
    "    y(t) &\\approx y\\left(t+\\frac{\\delta}{2}\\right) + \\frac{\\delta}{2}y'\\left(t+\\frac{\\delta}{2}\\right) + \\frac{\\delta^2}{8}y''\\left(\\frac{\\delta}{2}\\right) + \\frac{\\delta^3}{48}y'''(\\eta_+), \\hspace{0.5cm}\\text{and}\\\\\n",
    "    y(t+\\delta) &\\approx y\\left(t+\\frac{\\delta}{2}\\right) - \\frac{\\delta}{2}y'\\left(t+\\frac{\\delta}{2}\\right) + \\frac{\\delta^2}{8}y''\\left(\\frac{\\delta}{2}\\right) - \\frac{\\delta^3}{48}y'''(\\eta_-)\n",
    "\\end{align}$\n",
    "\n",
    "with $\\eta_+\\in [t, t+\\frac{\\delta}{2}]$ and $\\eta_+\\in [t+\\frac{\\delta}{2}, t+\\delta]$.\n",
    "\n",
    "Taking the difference of both:\n",
    "\n",
    "$\\begin{align}\n",
    "    y(t+\\delta) - y(t) &= \\delta y'\\left(t + \\frac{\\delta}{2}\\right) +  \\frac{\\delta^3}{24}y'''(\\eta)\\\\\n",
    "    \\frac{y(t+\\delta)-y(t)}{\\delta} &= y'\\left(t + \\frac{\\delta}{2}\\right) +  \\frac{\\delta^2}{24}y'''(\\eta)\n",
    "\\end{align}$\n",
    "\n",
    "with $\\eta\\in[t, t+\\delta]$. While using the standard difference quotient (as in Euler's method) yields an error that is proportional to $\\delta$, a method based on the central difference quotient will produce errors proportional to $\\delta^2$, which is much smaller (considering $\\delta<1$). This is the idea behind the method of Runge.\n",
    "\n",
    "### Runge's method\n",
    "\n",
    "Use the central difference quotient to implement a timestepping method:\n",
    "\n",
    "$\\begin{align}\n",
    "    \\frac{y(t+\\delta)-y(t)}{\\delta} &\\approx y'\\left(t+\\frac{\\delta}{2}\\right)\\\\\n",
    "    \\Leftrightarrow y(t+\\delta) &\\approx y(t) + \\delta y'\\left(t+\\frac{\\delta}{2}\\right)\n",
    "\\end{align}$\n",
    "\n",
    "Of course we do not know $y\\left(t+\\frac{\\delta}{2}\\right)$, but we can simply approximate it with an explicit Euler step of size $\\frac{\\delta}{2}$. From the second step onwards, we can re-use the values we already calculated in the previous step.\n",
    "\n",
    "##### EXERCISE\n",
    "\n",
    "Implement a single step of Runge's method for the continuous-time cobweb model! You will have to calculate the midpoint state by implementing a single Euler step of size $\\delta/2$ to calculate the next step. you can use the function you created above for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6d61e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runge_1step(p, delta):\n",
    "    # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c19ff4c",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "\n",
    "Ultimately, we are not interested in tiny steps of a model, but longer time series and the convergence to equilibrium, or divergence to $\\pm\\infty$. Hence, in the next step, your task will be to implement many of those single steps in a row. Both functions for single steps are using the same structure of inputs `p` and `delta`, and an output of the price after one incremental time step, so we can write a function that can flexibly apply either timestepping method.\n",
    "\n",
    "##### EXERCISE\n",
    "\n",
    "Complete the function below to simulate the cobweb model. They both take an initial value `p`, the number of unit steps `T`, and the number of increments per unit step, `n`. Hint: $\\delta=1/(n+1)$. They should not return a whole timeseries of values, but only the final value at time $T$. The final input is a function object, `timestep_func`. In Python (as in many other programming languages), you can pass a function as an input to another function. We do not evaluate it (no parentheses), but we apply it later in the code block of the function we are writing in the following way:\n",
    "\n",
    "```python\n",
    "def my_func(other_func, a):\n",
    "    other_func(a)\n",
    "```\n",
    "\n",
    "In the same way, you can run the timestep_func with inputs `p` and `delta` inside the function that simulates a series of steps, `simulate_cobweb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b517525c",
   "metadata": {},
   "source": [
    "##### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_cobweb(p, T, n, timestep_func):\n",
    "    # TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analytical solution\n",
    "def cobweb_1period(p_0, T):\n",
    "    return (p_0 - 15) * np.exp(-3*T) + 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61854f59",
   "metadata": {},
   "source": [
    "Below we compare the behaviour of both methods in terms of their errors and convergence towards the true value, as calculated with the analytical solution (see Tutorial). We iterate through decreasing values of $\\delta\\in\\{2^{-2}, 2^{-3}, \\dots 2^{-10}\\}$, halving it every step, i.e. doubling the amount of time increments per unit time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416aa93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = 60\n",
    "T = 1\n",
    "p_analytical = cobweb_1period(p0, T)\n",
    "\n",
    "# errors\n",
    "ee_error = 1\n",
    "rg_error = 1\n",
    "\n",
    "# analyse convergence, doubling the number of steps per time unit\n",
    "for exponent in range(2,11):\n",
    "    n = 2 ** exponent\n",
    "    # update last period errors\n",
    "    ee_last_error = ee_error\n",
    "    rg_last_error = rg_error\n",
    "    \n",
    "    # starting prices\n",
    "    p_ee = p_rg = p0\n",
    "    \n",
    "    # calculate solutions\n",
    "    p_ee = simulate_cobweb(p_ee, T, n, exp_euler_1step)\n",
    "    p_rg = simulate_cobweb(p_rg, T, n, runge_1step)\n",
    "    \n",
    "    # new errors\n",
    "    ee_error = p_ee - p_analytical\n",
    "    rg_error = p_rg - p_analytical\n",
    "    \n",
    "    # convergence\n",
    "    print(f'delta = 1/{n}:')\n",
    "    print(\n",
    "        f'Explicit Euler scheme \\n current value: p = {p_ee:.3e}; error = {ee_error:.3e}; error ratio = {ee_last_error / ee_error:.2e}'\n",
    "    )\n",
    "    print(\n",
    "        f'Central difference scheme \\n current value: p = {p_rg:.3e}; error = {rg_error:.3e}; error ratio = {rg_last_error / rg_error:.2e}'\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74967356",
   "metadata": {},
   "source": [
    "If your code is correct, you will notice that the errors decrease much faster for the Runge method, i.e. the method converges to the true value faster. By halving $\\delta$, we can also cut the errors in half if we use the Eueler scheme, but we make it four times smaller using the Runge method. That is because the convergence is quadratic instead of linear. We perform twice as many computationally steps per time increment $\\delta$ because we have to compute the midpoint, but because of the faster convergence, we still reach the same level of accuracy with lower computational cost. The error of the standard difference method at $\\delta = 1/1024$ is approximately 0.001, which has been approximately reached at $\\delta =1/64$ with the central differences method!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f09f37",
   "metadata": {},
   "source": [
    "### A note on higher-order equations\n",
    "\n",
    "In this tutorial, we are only dealing with a model based on a single first-order differential equation. The timestepping methods here are ideal for this, and instead of extending them to higher-order equations, we will transform those into systems of first-order equations, which are going to be the topic of the next tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf06f203",
   "metadata": {},
   "source": [
    "### Simulating time series\n",
    "\n",
    "In the last simulation exercise, we were only interested in calculating the value of the state variable $p$ after a number of time steps $T$. Often, we are however interested in the time series of the model, starting from a given initial value. To practice the two timestepping schemes a bit more, we will introduce a different first-order differential equation, simulate it over time, and finally plot the result.\n",
    "\n",
    "##### EXERCISE\n",
    "\n",
    "Consider the equation $y' + 3y = 2$. Complete all the functions below: single incremental steps using the Euler and Runge methods, and a function that can apply either of these methods to simulate a time series (the output should be of type `list`) for a given number of time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d999e1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ee_1step(y, delta):\n",
    "    # TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab933aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runge_1step(y, delta):\n",
    "    y_mid = ee_1step(y, 0.5*delta)\n",
    "    # TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(y0, T, n, timestep_func):\n",
    "    # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d4788b",
   "metadata": {},
   "source": [
    "Below, we simulate the model for 3 steps from initial value 5, using a relatively large increment $\\delta=1/4$, and then we plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = 5\n",
    "T = 3\n",
    "n = 4\n",
    "\n",
    "# results using Euler\n",
    "ee_res = simulate(y0, T, n, ee_1step)\n",
    "# results using Runge\n",
    "rg_res = simulate(y0, T, n, runge_1step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"$y'(t) + 3y(t) = 2$, $y_0=5$\")\n",
    "\n",
    "# values for the x-axis\n",
    "x = np.linspace(0, T, T*n+1)\n",
    "\n",
    "# plot the exact solution too\n",
    "A = y0 - (2/3)\n",
    "x_exact = np.linspace(0, T, 1000) # more points for a smoother curve\n",
    "y_exact = A * np.exp(-3 * x_exact) + (2/3)\n",
    "\n",
    "plt.plot(x, ee_res, label=\"Explicit Euler\", lw=0.7, c=\"darkblue\")\n",
    "plt.plot(x, rg_res, label=\"Runge\", lw=0.7, c=\"darkgreen\")\n",
    "plt.plot(x_exact, y_exact, label=\"analytical solution\", lw=0.7, c='red')\n",
    "\n",
    "plt.hlines(2/3, 0, T, color='black', lw=0.7, ls='--')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f63c87",
   "metadata": {},
   "source": [
    "Note how the distance from the green (upper) line to the exact solution (red line in the middle) is much smaller than the distance of the Euler method to the analytical solution. Moreover, you can clearly see another property of the Euler method: In the case of a convex function $\\frac{\\partial^2}{\\partial t^2}>0$ as the present one, the slope is exaggerated, i.e. in the case of a positive (negative) coefficient, we would obtain a positive (negative) bias. We only approximate the slope at the current point and and take a step $dt=\\delta$ along this slope. However, the slope is changing (non-zero second derivative, i.e. the rate of change is changing), and therefore at any point between $t$ and $t+\\delta$, the slope would already be different, which we ignore, leading to small approximation errors. The central difference methodobtains a more accurate representation of the true behaviour of the system, as it applies the slope in the middle, which is closer to the average slope over the entire distance.\n",
    "\n",
    "You can play around with the values of $n, \\delta$ and see how they change the behaviour of the approximation. For values $n>10$ it the difference between Runge's method and the exact solution quickly become invisible, while it takes a bit longer with the EUler method. You might also note a curious error source for $n\\leq2$, which we will address in the next tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd915bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
