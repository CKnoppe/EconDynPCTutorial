{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa3a0a3",
   "metadata": {},
   "source": [
    "# 5 Nonlinear Dynamics\n",
    "\n",
    "So far we have only simulated linear models, which are of course only a small subset of all possible dynamic models. As you have seen in the lectures and tutorials, solving models can become much more complicated when non-linearities are involved, and is often restricted to local analyses of linearised systems around fixed points. Simulating non-linear models however is is not so different to simulating linear models. The only complication is that the fixed point problem that arises in implicit models may not always be straightforward to solve.\n",
    "\n",
    "In this tutorial you will learn how to circumvent the problem of unsolvable fixed points and we apply it to the Crank-Nicolson method. Moreover, we will touch upon numerical methods to find fixed points of generic dynamic models using Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa32e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "\n",
    "from matplotlib import rc\n",
    "rc(\"text\", usetex=True) # Latex font in figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe83f86",
   "metadata": {},
   "source": [
    "### 5.1 Solow model with constant technology and population\n",
    "\n",
    "Consider Solow's growth model, defined by the following equations:\n",
    "\n",
    "$\\begin{align}\n",
    "    Y &= AK^{\\alpha}L^{(1-\\alpha)} \\hspace{0.1cm};\\hspace{0.5cm}0<\\alpha<1\\\\\n",
    "    Y &= C + I \\\\\n",
    "    S &= sY = I\\\\\n",
    "    K' &= I - \\gamma K = sY\n",
    "\\end{align}$\n",
    "\n",
    "The first equation is a standard Cobb-Douglas production function with technology parameter $A$, capital input $K$, and labour input $L$, which defines the total output $Y$. The second equation states that all produciton is either consumed or invested, and the third equation is the savings-investment identity $S=I$. Note that a fixed share of total income $s$ is saved in every period. Finally, The law of motion states that the rate of change of capital equals investments (positive inflow to capital) minus depreciation (outflow form capital, through wear-and-tear or similar), which is a linear function of capital with the depreciation rate $\\gamma$.\n",
    "\n",
    "In per-capita terms, the last equation can be restated as\n",
    "\n",
    "$\\begin{equation}\n",
    "    k' = sk^{\\alpha} - \\gamma k\n",
    "\\end{equation}$\n",
    "\n",
    "Solving the fixed-point problem of the implicit Crank-Nicolson method to iterate this equation forward in time would be cumbersome at best. Instead, we will approximate $k_{t+\\delta}$ with an explicit Euler step, and calculate the rate of change at this point, so we can use it in our Crank-Nicolson scheme. This is not as accurate as the analytical solution, but still improves a lot on the explicit Euler scheme applied by itself.\n",
    "\n",
    "##### EXERCISE\n",
    "\n",
    "Implement a single step of the Crank-Nicolson method in the function below. As explained above, estimate $k_{t+\\delta}$ with an explicit Euler step and then calculate the derivates at $k_t$ and $k_{t+\\delta}$ to perform the step forward. \n",
    "\n",
    "Write a separate function for the Euler step. That way, we can compare behaviour of both methods afterwards, and your code becomes more readable. Generally, it is a good idea to split functions into subroutines, so that each function has narrow and focused responsibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fcb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# euler step\n",
    "def ee_solow_step(k, alpha, s, gamma, delta):\n",
    "    # TODO\n",
    "\n",
    "# crank-nicolson\n",
    "def cn_solow_step(k, alpha, s, gamma, delta):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16e28f",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "Implement the function that simulates Solow's model with either of the two timestepping functions!\n",
    "\n",
    "Inputs are the standard model parameters, the time increment $\\delta$, as well as the total number of *unit* time steps `T` and the timestepping function. Output should be a numpy array of capital values $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a65a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_Solow(k_0, alpha, s, gamma, delta, T, timestep_func):\n",
    "    # total number of time increments: inverse of increment times unit steps\n",
    "    T = int(T/delta)\n",
    "    \n",
    "    # time series of capital, as an array\n",
    "    k_ts = np.empty(T+1)\n",
    "    k_ts[0] = k_0\n",
    "    \n",
    "    for t in range(T):\n",
    "        # TODO\n",
    "        \n",
    "    return k_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a62bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variable values\n",
    "delta = 1/16 # time increment\n",
    "T = 75        # time unit steps\n",
    "alpha = 1/3   # Cobb-Douglas exponent\n",
    "gamma = 0.1   # depreciation rate of capital\n",
    "s = 0.1       # savings rate\n",
    "k_0 = 0.1     # initial capital\n",
    "\n",
    "Solow_T_CN = simulate_Solow(k_0, alpha, s, gamma, delta, T, cn_solow_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d406e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.title('Solow growth model implementation')\n",
    "\n",
    "# x-axis re-adjusted\n",
    "t = np.linspace(0, T, int(T/delta)+1)\n",
    "# plot the time series\n",
    "plt.plot(t, Solow_T_CN, lw=0.8)\n",
    "\n",
    "# analytical solution of the steady state:\n",
    "ss = (s / gamma) ** (1 / (1-alpha))\n",
    "# horizontal line at steady state\n",
    "plt.hlines(ss, 0, T, ls='--', lw=0.5, color=\"black\", alpha=0.8)\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e122a",
   "metadata": {},
   "source": [
    "##### Bonus exercise\n",
    "\n",
    "Implement the logistic growth function $x' = rx(1-x)$ using the Crank-Nicolson method!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af44fa4f",
   "metadata": {},
   "source": [
    "### Systems of non-linear equations\n",
    "\n",
    "Consider the system of Tutorial 5, exercise 1:\n",
    "\n",
    "$\\begin{align}\n",
    "    z_1' &= z_2(z_1 + 1)\\\\\n",
    "    z_2' &= z_1(z_2 + 3)\n",
    "\\end{align}$\n",
    "\n",
    "Implementing simulation methods for systems of non-linear equations should bring no surprises to you: instead of calculating the derivative for one variable, we calculate the derivatives of all state variables, and increment all variables according to their law of motion.\n",
    "\n",
    "##### EXERCISE\n",
    "Write three separate functions:\n",
    "* one that takes the current state vector (numpy array) as input and returns the derivative at that point (also as a numpy array)\n",
    "* one that implements a single explicit Euler step\n",
    "* and finally, one that implements a single Crank-Nicolson step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be701f0c",
   "metadata": {},
   "source": [
    "##### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24e492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(z):\n",
    "    # TODO\n",
    "\n",
    "def ee_step(z, delta):\n",
    "    # TODO\n",
    "\n",
    "def cn_step(z, delta):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51649fe6",
   "metadata": {},
   "source": [
    "Finally, we can simulate this system too.  Let's try different initial conditions to see how it does not always reach the same steady state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850d43ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1/2014         # time increment\n",
    "\n",
    "# two different time lengths for stable and unstable starting positions (for plotting purposes)\n",
    "T_stable = 2\n",
    "T_unstable = 1\n",
    "\n",
    "# different initial conditions\n",
    "# stable\n",
    "za = np.array([1, -2]) \n",
    "zb = np.array([-4, -4])\n",
    "zc = np.array([-0.75, 2])\n",
    "# unstable\n",
    "zd = np.array([2, -1.52])\n",
    "ze = np.array([-0.5, 2])\n",
    "\n",
    "z1 = np.empty((2,int(T_stable/delta) + 1))\n",
    "z1[:, 0] = za\n",
    "\n",
    "z2 = np.empty((2,int(T_stable/delta) + 1))\n",
    "z2[:, 0] = zb\n",
    "\n",
    "z3 = np.empty((2,int(T_stable/delta) + 1))\n",
    "z3[:, 0] = zc\n",
    "\n",
    "z4 = np.empty((2,int(T_unstable/delta) + 1))\n",
    "z4[:, 0] = zd\n",
    "\n",
    "z5 = np.empty((2,int(T_unstable/delta) + 1))\n",
    "z5[:, 0] = ze\n",
    "\n",
    "U_stable = int(T_stable/delta)\n",
    "U_unstable = int(T_unstable/delta)\n",
    "\n",
    "for u in range(U_stable):\n",
    "    z1[:, u+1] = cn_step(z1[:, u], delta)\n",
    "    z2[:, u+1] = cn_step(z2[:, u], delta)\n",
    "    z3[:, u+1] = cn_step(z3[:, u], delta)\n",
    "    \n",
    "for u in range(U_unstable):\n",
    "    z4[:, u+1] = cn_step(z4[:, u], delta)\n",
    "    z5[:, u+1] = cn_step(z5[:, u], delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec21c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for z in [z1, z2, z3, z4, z5]:\n",
    "    plt.plot(z[0], z[1], lw=0.7, c='black')\n",
    "    # add arrow heads\n",
    "    dz1 = z[0,-1] - z[0,-2]\n",
    "    dz2 = z[1, -1] - z[1, -2]\n",
    "    plt.arrow(z[0,-2], z[1, -2], dz1, dz2, head_width=0.075)\n",
    "\n",
    "# steady states\n",
    "plt.scatter([0, -1],[0, -3], s=15, c='black')\n",
    "\n",
    "# give the figure a more \"mathy\" feel by removing the top and left spines, \n",
    "# add arrowheads to the others and make them the z1 and z2 axes of the coordinate system\n",
    "ax = plt.gca()\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "arrow_fmt = dict(markersize=4, color='black', clip_on=False)\n",
    "ax.plot((1), (0), marker='>', transform=ax.get_yaxis_transform(), **arrow_fmt)\n",
    "ax.plot((0), (1), marker='^', transform=ax.get_xaxis_transform(), **arrow_fmt)\n",
    "\n",
    "plt.grid(ls=':', lw=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec196c4f",
   "metadata": {},
   "source": [
    "This figure shows the saddle stability of the steady state in the origin quite nicely. \n",
    "\n",
    "A good alternative to phase diagrams for non-linear systems are gradient fields. Those draw little arrows at discrete points of the grid, representing the direction the system takes if it starts from there. Find an implementation using the derivatives function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad944cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridpoints on both axes\n",
    "z1_ax = np.linspace(-3, 2, 20)\n",
    "z2_ax = np.linspace(-4, 3, 20)\n",
    "\n",
    "# create a 3d grid: one dimension for each axis\n",
    "# 1 dimension because derivatives come in two values (dz1, dz2)\n",
    "grid = np.empty((2, len(z1_ax), len(z2_ax)))\n",
    "\n",
    "# fill grid\n",
    "for i, z1_ in enumerate(z1_ax):\n",
    "    for j, z2_ in enumerate(z2_ax):\n",
    "        grid[:,i,j] = derivative(np.array([z1_, z2_]))\n",
    "        \n",
    "# colours will be based on magnitude of change\n",
    "# create a grid with derivatives where total change is scaled between 0 and 1\n",
    "col_grid = np.empty((len(z1_ax), len(z2_ax)))\n",
    "for i in range(len(z1_ax)):\n",
    "    for j in range(len(z2_ax)):\n",
    "        col_grid[i,j] = np.sqrt(grid[0,i,j]**2 + grid[1,i,j]**2)\n",
    "# scaling with logistic function - created better distribution of hot and cold colours in my opinion\n",
    "col_grid = 1 / (1 + np.exp(-0.5*(col_grid - col_grid.mean())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf011e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "cmap = colormaps['coolwarm']\n",
    "\n",
    "for i, z1_ in enumerate(z1_ax):\n",
    "    for j, z2_ in enumerate(z2_ax):\n",
    "        dz1, dz2 = grid[:, i, j]\n",
    "        # scale the length of the arrow\n",
    "        length = np.sqrt(dz1**2 + dz2**2)\n",
    "        dz1_scaled = 0.15 * dz1 / length\n",
    "        dz2_scaled = 0.15 * dz2 / length\n",
    "        \n",
    "        # color according to the magnitude of change\n",
    "        col = cmap(col_grid[i,j])\n",
    "\n",
    "        plt.arrow(\n",
    "            z1_, z2_, dz1_scaled, dz2_scaled, \n",
    "            color=col, lw=0.7, head_width=0.035, length_includes_head=True\n",
    "        )\n",
    "\n",
    "# steady states\n",
    "plt.scatter([0, -1],[0, -3], s=15, c='black')\n",
    "\n",
    "plt.grid(lw=0.3, ls=':', color=\"black\", alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc4747e",
   "metadata": {},
   "source": [
    "Note how I scaled the magnitudes of change (length of the vectors by the logistic function\n",
    "\n",
    "$\\begin{equation}\n",
    "    \\tilde{x} = \\frac{1}{1 + exp(-0.5(x-\\bar{x}))}\n",
    "\\end{equation}$\n",
    "\n",
    "The primary reason was to rescale the lengths so that they lie within $\\tilde{x}\\in[0,1]$, because that can be used by the colormap I am applying to create a colour scheme. I could have used a more simple regularization method, such as\n",
    "\n",
    "$\\begin{equation}\n",
    "    \\tilde{x} = \\frac{x - min(x)}{max(x) - min(x)}\n",
    "\\end{equation}$\n",
    "\n",
    "However, upon trying it out, I found the distribution of colours quite uninformative - try it yourself, if you are interested. The logistic scaling created a more interesting colour scheme.\n",
    "\n",
    "The \"coolwarm\" colormap provides a quite intuitive scheme for magnitudes of change, but if you have issues such as colour blindness, feel free to try some alternatives (\"viridis\", \"jet\", \"hot\", \"gist_heat\", \"gnuplot\", \"rainbow\", \"plasma\", \"inferno\", \"terrain\", \"ocean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1c46a",
   "metadata": {},
   "source": [
    "### Finding fixed points\n",
    "\n",
    "Another common problem in many numerical applications is the detection of fixed points or determining roots (zeros). Other examples include optimisation, i.e. finding values of a variable at which some function has a zero-derivative. Values at which a function equals zero also define the fixed points of a dynamic model, so this is a good opportunity to look into it.\n",
    "\n",
    "One of the simplest, yet still a powerful method is Netwon's method. The method is iterative, i.e. we start from a random \"guess\" and then update that guess, try from that updated guess again, and so on. The specific updating rule is as follows:\n",
    "\n",
    "$\\begin{equation}\n",
    "    x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n",
    "\\end{equation}$\n",
    "\n",
    "Intuitively that means that we follow the tangent line of the function at our current guess and set our next guess to be the root of that tangent line. See the figure below to illustrate this updating mechanism. Then we can perform this step over and over again to approximate the roots of a function, without having to calculate it by hand. We have to implement a stopping criterion of cpurse, e.g. we break out of this process once we have found a value at which the function is close enough to zero.\n",
    "\n",
    "In the case of higher dimensions we have to replace derivatives with the Jacobian matrix, but otherwise it is the same. Consider  system of $n$ state variables, denoted as a vector $\\mathbf{x}$, and the dynamics are described by the vector-valued function $F: \\mathbb{R}^n\\rightarrow\\mathbb{R}^n$, i.e. $\\mathbf{x}' = F(\\mathbf{x})$. The updating scheme to find $\\mathbf{x}'=0$ is then\n",
    "\n",
    "$\\begin{equation}\n",
    "    \\mathbf{x}_{n+1} = \\mathbf{x}_n - J_F(\\mathbf{x_n})^{-1}F(\\mathbf{x_n})\n",
    "\\end{equation}$\n",
    "\n",
    "where $J_F(\\mathbf{x_n})$ is the Jacobian of $F$ at $\\mathbf{x}_n$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45569e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"One step of Newton's method\")\n",
    "ax = plt.gca()\n",
    "\n",
    "# the function of which we want to find the roots\n",
    "def func(x):\n",
    "    return 0.5*x**3 - 0.5\n",
    "\n",
    "x0 = 2.5\n",
    "\n",
    "x = np.linspace(-1, 3, 1000)\n",
    "y = 0.5*x**3 - 0.5\n",
    "# tangent slope at x=2\n",
    "slope = 1.5 * x0**2\n",
    "intercept = func(x0) - slope * x0\n",
    "tangent = slope*x[500:] + intercept\n",
    "\n",
    "plt.plot(x, y, lw=0.7, c='black', label='function')\n",
    "plt.plot(x[500:], tangent, lw=0.7, c='red', label='tangent')\n",
    "\n",
    "plt.vlines(x0, 0, func(x0), color='black', lw=0.7, ls='--')\n",
    "\n",
    "x1 = x0 - func(x0) / slope\n",
    "\n",
    "# custom ticks and ticklabels to include x_0 and x_1\n",
    "xticks_major = list(range(-1, 4)) \n",
    "xticks_minor = [x0, x1]\n",
    "xticklabels = [\"$x_0$\", \"$x_1$\"]\n",
    "plt.xticks(xticks_major)\n",
    "ax.set_xticks(ticks=xticks_minor, labels=xticklabels, minor=True)\n",
    "\n",
    "### stylistic stuff\n",
    "plt.text(2.2, 7.5, \"$f(x_0)$\")\n",
    "\n",
    "plt.grid(lw=0.3, ls=':')\n",
    "\n",
    "# give the figure a more \"mathy\" feel by removing the top and left spines, \n",
    "# add arrowheads to the others and make them the z1 and z2 axes of the coordinate system\n",
    "ax = plt.gca()\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "arrow_fmt = dict(markersize=4, color='black', clip_on=False)\n",
    "ax.plot((1), (0), marker='>', transform=ax.get_yaxis_transform(), **arrow_fmt)\n",
    "ax.plot((0), (1), marker='^', transform=ax.get_xaxis_transform(), **arrow_fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed313c",
   "metadata": {},
   "source": [
    "##### EXERCISE\n",
    "\n",
    "Consider Solow's model of economic growth as an example again: $k' = f(k) = sk^{\\alpha} - \\gamma k$. We know how to find the steady states of the model, but let us implement Newton's method to let the computer find it for us. Fill out the remaining parts of the functions below. `newton_solow_1step` implements one iteration of the method for the Solow model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6dc665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_prime(k, alpha, s, gamma):\n",
    "    '''\n",
    "    calculate the rate of change of capital given the current amount of capital k' = f(k)\n",
    "    '''\n",
    "    # TODO\n",
    "\n",
    "def k_doubleprime(k, alpha, s, gamma):\n",
    "    '''\n",
    "    calculate the derivative of the rate of change of capital k'' = f'(k)\n",
    "    '''\n",
    "    # TODO\n",
    "\n",
    "def newton_solow_1step(k, alpha, s, gamma):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfcca72",
   "metadata": {},
   "source": [
    "##### EXERCISE\n",
    "\n",
    "Implement the remaining parts of the iterative procedure in the function below. Inputs `k`, `alpha`, `s`, `gamma` are well-known by now. `max_iter` specifies the maximum number of iterations we perform before \"giving up\" and `crit` is the error that we consider small enough to be satisfied with the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b82a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_solow(k, alpha, s, gamma, max_iter=1000, crit=1e-6):\n",
    "    for i in range(max_iter): # even if we do not converge, we have to stop eventually\n",
    "        # TODO: \n",
    "        #    - newton step\n",
    "        #    - check if we have converged after every step\n",
    "     \n",
    "    # finally, we return our estimate\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c8962",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.3   # Cobb-Douglas exponent\n",
    "gamma = 0.1   # depreciation rate of capital\n",
    "s = 0.2       # savings rate\n",
    "k0 = 0.5      # initial capital\n",
    "\n",
    "print(f'steady state found at k={round(newton_solow(k0, alpha, s, gamma),4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cbeb76",
   "metadata": {},
   "source": [
    "Note how the function actually has two steady states: the one you already know, and the one in the origin. Multiple steady states often occur in non-linear systems, and since initial guesses are typically random, it is important to try different starting points!\n",
    "\n",
    "##### ADDITIONAL EXERCISE\n",
    "\n",
    "Implement Newton's method on the 2-dimensional system \n",
    "\n",
    "$\\begin{align}\n",
    "    z_1' &= z_2(z_1 + 1)\\\\\n",
    "    z_2' &= z_1(z_2 + 3)\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fd3cd2",
   "metadata": {},
   "source": [
    "### Bonus: Numerical differencing\n",
    "\n",
    "Sometimes the derivative may not be easy to calculate, or you don't know the exact function of the dynamic model you are analysing That can be particularly relevant when you are working with empirical data for which you d not know the exact data generating function. In that case, numerical differencing comes in handy. Consider the definition of derivatives:\n",
    "\n",
    "$f'(x) = \\underset{h\\rightarrow0}{lim}\\frac{f(x+h) - f(x)}{h}$. \n",
    "\n",
    "We cannot actually implement infinitesimally small steps, but we can approximate derivatives with finite differences, using finitely small values of $h$:\n",
    "\n",
    "$f'(x)\\approx\\frac{f(x+h) - f(x)}{h}; \\hspace{0.5cm}h>0$.\n",
    "\n",
    "We can use forward or backwards differencing ($+h$ or $-h$), but a good option is usually central differences:\n",
    "\n",
    "$f'(x)\\approx\\frac{f(x+\\frac{h}{2}) - f(x-\\frac{h}{2})}{h}$.\n",
    "\n",
    "Since we have to evaluate the function at two points anyway, this does not come at a significantly higher computational cost either.\n",
    "\n",
    "##### BONUS EXERCISE\n",
    "\n",
    "Implement the Newton method on the Solow model again, but now use finite difference approximations instead of the analytical derivative. I.e. You do not use the function `k_doubleprime()`, but only the differences in values of `k_prime()` to approximate derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958ea7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
